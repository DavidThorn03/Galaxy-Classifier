{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac",
      "metadata": {
        "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac"
      },
      "source": [
        "## Classifier per  Base Level Model\n",
        "with resnet18, classifier per Level, and functions to combine coarse to fine probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6",
      "metadata": {
        "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
      "metadata": {
        "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62cd32b-4d98-470b-a9bf-5d64186a3f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hiclass[all]\n",
            "  Downloading hiclass-5.0.4-py3-none-any.whl.metadata (16 kB)\n",
            "\u001b[33mWARNING: hiclass 5.0.4 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.5 in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5->hiclass[all]) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5->hiclass[all]) (3.6.0)\n",
            "Downloading hiclass-5.0.4-py3-none-any.whl (50 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hiclass\n",
            "Successfully installed hiclass-5.0.4\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install hiclass[all]\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import color\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from hiclass.metrics import f1 as hf1\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044c180a-d6d9-4bf8-9545-4f4147733a61",
      "metadata": {
        "id": "044c180a-d6d9-4bf8-9545-4f4147733a61"
      },
      "source": [
        "### LBP Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53000783-494e-44d7-88c7-0900c1fdfafc",
      "metadata": {
        "id": "53000783-494e-44d7-88c7-0900c1fdfafc"
      },
      "outputs": [],
      "source": [
        "class LBPTransform:\n",
        "    def __init__(self, radius=3, n_points=None, method='uniform'):\n",
        "        self.radius = radius\n",
        "        self.n_points = n_points if n_points else 8 * radius\n",
        "        self.method = method\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        if len(img.shape) == 3 :\n",
        "            gray = color.rgb2gray(img)\n",
        "        else:\n",
        "            gray = img\n",
        "\n",
        "        gray = (gray * 255).astype(np.uint8)\n",
        "\n",
        "        lbp = local_binary_pattern(gray, self.n_points, self.radius, self.method)\n",
        "\n",
        "        lbp = (lbp - lbp.min()) / (lbp.max() - lbp.min() + 1e-7)\n",
        "\n",
        "        lbp_3 = np.stack([lbp, lbp, lbp], axis=-1)\n",
        "\n",
        "        return lbp_3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1",
      "metadata": {
        "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1"
      },
      "source": [
        "### Make LBP Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1",
      "metadata": {
        "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1"
      },
      "outputs": [],
      "source": [
        "def make_lbp_csv(input_folder, csv_path, lbp_transformer):\n",
        "    img_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    print(f\"Processing {len(img_files)} images from {input_folder}...\")\n",
        "    with open(csv_path, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        header_written=False\n",
        "\n",
        "        for img, fname in enumerate(img_files):\n",
        "          in_path = os.path.join(input_folder, fname)\n",
        "          img = Image.open(in_path).convert('RGB')\n",
        "\n",
        "          img = lbp_transformer(img)\n",
        "          img = img.flatten()\n",
        "\n",
        "          if not header_written:\n",
        "            header = ['PGCname'] + [f'pixel_{i}' for i in range(len(img))]\n",
        "            writer.writerow(header)\n",
        "            header_written = True\n",
        "\n",
        "          writer.writerow([fname] + img.tolist)\n",
        "\n",
        "    print(\"LBP preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3",
      "metadata": {
        "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3"
      },
      "source": [
        "### Dataset Class\n",
        "Class for processing data and combining images with labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479",
      "metadata": {
        "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479"
      },
      "outputs": [],
      "source": [
        "class PGCDataset(Dataset):\n",
        "    def __init__(self, labels_df, img_folder, id_col='PGCname', label_col='T', transform=None):\n",
        "        self.labels_df = labels_df.reset_index(drop=True)\n",
        "        self.img_folder = img_folder\n",
        "        self.id_col = id_col\n",
        "        self.label_col = label_col\n",
        "        self.transform = transform\n",
        "\n",
        "        available_imgs = {f.replace('.png', '') for f in os.listdir(img_folder)\n",
        "                            if f.endswith('.png')}\n",
        "        self.labels_df = self.labels_df[self.labels_df[id_col].isin(available_imgs)].reset_index(drop=True)\n",
        "\n",
        "        print(f\"Dataset created with {len(self.labels_df)} imgs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.labels_df.iloc[idx]\n",
        "\n",
        "        img_id = row[self.id_col]\n",
        "        img_path = os.path.join(self.img_folder, f\"{img_id}.png\")\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        label = torch.tensor(int(row[self.label_col]), dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label, img_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5",
      "metadata": {
        "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
      "metadata": {
        "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38da133-64fd-47a9-c124-9d51ed786ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 3120 imgs\n",
            "Dataset created with 446 imgs\n",
            "Dataset created with 892 imgs\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/Othercomputers/My laptop/Thesis/Galaxy-Classifier/'\n",
        "img_folder = path + '/images'\n",
        "lbp_img_folder = path + '/lbp_images'\n",
        "\n",
        "id_col = 'PGCname'\n",
        "label_col = 'T'\n",
        "\n",
        "img_size = 224\n",
        "\n",
        "lbp_trans = LBPTransform(5)\n",
        "\n",
        "labels_df = pd.read_csv(path + 'EFIGI_attributes.txt', sep=r'\\s+', comment='#')\n",
        "labels_df[label_col] = labels_df[label_col].replace({-3:-2, -1:-2}) # S0\n",
        "labels_df[label_col] = labels_df[label_col].replace({0:1, 2:1}) # Sa\n",
        "labels_df[label_col] = labels_df[label_col].replace({3:4}) # Sb\n",
        "labels_df[label_col] = labels_df[label_col].replace({5:6}) # Sc\n",
        "labels_df[label_col] = labels_df[label_col].replace({8:7, 9:7}) # Sd\n",
        "labels_df[label_col] = labels_df[label_col].replace({10:11}) # Irr\n",
        "\n",
        "labels_df[label_col] = labels_df[label_col].replace({-6:0, -5:1, -4:2, -2:3, 1:4, 4:5, 11:8}) # Adjust to 0 - 8\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=0, stratify=labels_df[label_col])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.125, random_state=0, stratify=train_df[label_col])\n",
        "\n",
        "# use stratify sampling in training - write to csv file - - tocsv.pandas\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(180),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\"\"\"lbp_params = {'radius': 3, 'n_points': 24, 'method': 'uniform'}\n",
        "\n",
        "lbp_transform = LBPTransform(**lbp_params)\n",
        "\n",
        "make_lbp_images(img_folder, lbp_img_folder, lbp_transform)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_dataset = PGCDataset(\n",
        "    labels_df=train_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = PGCDataset(\n",
        "    labels_df=val_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")\n",
        "test_dataset = PGCDataset(\n",
        "    labels_df=test_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62",
      "metadata": {
        "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62"
      },
      "source": [
        "### Data loader\n",
        "loads data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26",
      "metadata": {
        "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26"
      },
      "outputs": [],
      "source": [
        "labels = train_df[label_col].values\n",
        "classes= np.unique(labels)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
        "\n",
        "sample_weights = np.array([class_weights[np.where(classes == label)[0][0]] for label in labels])\n",
        "sample_weights = torch.from_numpy(sample_weights).float()\n",
        "\n",
        "sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True) # worse for underrepresented classes and overall\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed",
      "metadata": {
        "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed"
      },
      "source": [
        "### Hierarchical model\n",
        "using pretrained resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146c13bc-506b-49d8-8b6e-095e8e170259",
      "metadata": {
        "id": "146c13bc-506b-49d8-8b6e-095e8e170259"
      },
      "outputs": [],
      "source": [
        "class HierarchicalResNet(nn.Module):\n",
        "  def __init__(self, num_coarse, num_fine, freeze_backbone=False):\n",
        "    super(HierarchicalResNet, self).__init__()\n",
        "    # get model\n",
        "    resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
        "    # freeze layers if needed\n",
        "    for param in resnet.parameters():\n",
        "      param.requires_grad = not freeze_backbone\n",
        "\n",
        "    # remove the final full connect layer\n",
        "    self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "    in_features = resnet.fc.in_features\n",
        "\n",
        "    # classifier heads\n",
        "    self.coarse_classifier = nn.Sequential(nn.Linear(in_features, num_coarse))\n",
        "    self.fine_classifier = nn.Sequential(nn.Linear(in_features, num_fine))\n",
        "\n",
        "  def forward(self, x):\n",
        "    features = self.backbone(x)\n",
        "    features = torch.flatten(features, 1)\n",
        "\n",
        "    # get predictions from features\n",
        "    coarse_output = self.coarse_classifier(features)\n",
        "    fine_output = self.fine_classifier(features)\n",
        "\n",
        "    return coarse_output, fine_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical Loss\n",
        "using cross entropy"
      ],
      "metadata": {
        "id": "zWmIxHc3JvDE"
      },
      "id": "zWmIxHc3JvDE"
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalLoss(nn.Module):\n",
        "  def __init__(self, fine_to_coarse_mapping, alpha=0.5, beta=0.3, fine_weights=None, coarse_weights=None):\n",
        "    super(HierarchicalLoss, self).__init__()\n",
        "    # hash map of fine - course\n",
        "    self.fine_to_coarse_mapping = fine_to_coarse_mapping\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "\n",
        "    self.loss_fine = nn.CrossEntropyLoss(weight=fine_weights)\n",
        "    self.loss_coarse = nn.CrossEntropyLoss(weight=coarse_weights)\n",
        "\n",
        "    # tensor for mapping\n",
        "    max_fine_label = max(fine_to_coarse_mapping.keys())\n",
        "    self.mapping_tensor = torch.zeros(max_fine_label + 1, dtype=torch.long)\n",
        "    for fine, coarse in fine_to_coarse_mapping.items():\n",
        "      self.mapping_tensor[fine] = coarse\n",
        "\n",
        "  def forward(self, coarse_logits, fine_logits, fine_labels, device):\n",
        "    self.mapping_tensor = self.mapping_tensor.to(device)\n",
        "    coarse_labels = self.mapping_tensor[fine_labels]\n",
        "\n",
        "    coarse_loss = self.loss_coarse(coarse_logits, coarse_labels)\n",
        "    fine_loss = self.loss_fine(fine_logits, fine_labels)\n",
        "\n",
        "    coarse_preds = torch.argmax(coarse_logits, dim=1)\n",
        "    fine_preds = torch.argmax(fine_logits, dim=1)\n",
        "    pred_coarse_from_fine = self.mapping_tensor[fine_preds]\n",
        "\n",
        "    # penalty when fine and coarse dont match\n",
        "    consistency_penalty = (coarse_preds != pred_coarse_from_fine).float().mean()\n",
        "\n",
        "    total_loss = fine_loss + self.alpha * coarse_loss + self.beta * consistency_penalty\n",
        "\n",
        "    return total_loss, coarse_loss, fine_loss, consistency_penalty"
      ],
      "metadata": {
        "id": "MKewVoSLJmK2"
      },
      "id": "MKewVoSLJmK2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate class weights"
      ],
      "metadata": {
        "id": "xd3-ttA8KBK3"
      },
      "id": "xd3-ttA8KBK3"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(labels, fine_to_coarse_mapping, device='cpu'):\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    fine_classes = np.unique(labels)\n",
        "    fine_weights = compute_class_weight('balanced', classes=fine_classes, y=labels)\n",
        "\n",
        "    max_fine_class = max(fine_to_coarse_mapping.keys())\n",
        "    fine_weights_full = torch.ones(max_fine_class + 1, dtype=torch.float, device=device)\n",
        "    for i, class_id in enumerate(fine_classes):\n",
        "        fine_weights_full[class_id] = fine_weights[i]\n",
        "\n",
        "    coarse_labels = np.array([fine_to_coarse_mapping[label] for label in labels])\n",
        "    coarse_classes = np.unique(coarse_labels)\n",
        "    coarse_weights = compute_class_weight('balanced', classes=coarse_classes, y=coarse_labels)\n",
        "\n",
        "    num_coarse_classes = max(fine_to_coarse_mapping.values()) + 1\n",
        "    coarse_weights_full = torch.ones(num_coarse_classes, dtype=torch.float, device=device)\n",
        "    for i, class_id in enumerate(coarse_classes):\n",
        "        coarse_weights_full[class_id] = coarse_weights[i]\n",
        "\n",
        "    return fine_weights_full, coarse_weights_full"
      ],
      "metadata": {
        "id": "B0JYvXK2KHZr"
      },
      "id": "B0JYvXK2KHZr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical F1 calculation"
      ],
      "metadata": {
        "id": "JwzuUh7PdJDC"
      },
      "id": "JwzuUh7PdJDC"
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_f1_per_class(true_hier, pred_hier, num_coarse, num_fine):\n",
        "    class_indices = defaultdict(list)\n",
        "    for i, (c, f) in enumerate(true_hier):\n",
        "        class_indices[(c, f)].append(i)\n",
        "\n",
        "    f1_per_class = {}\n",
        "\n",
        "    for (c, f), idxs in class_indices.items():\n",
        "        if len(idxs) == 0:\n",
        "            f1_per_class[(c, f)] = 0.0\n",
        "            continue\n",
        "\n",
        "        t_subset = [true_hier[i] for i in idxs]\n",
        "        p_subset = [pred_hier[i] for i in idxs]\n",
        "\n",
        "        score = hf1(t_subset, p_subset)\n",
        "        f1_per_class[(c, f)] = score\n",
        "\n",
        "    return f1_per_class\n"
      ],
      "metadata": {
        "id": "Hefcj_nxdOTz"
      },
      "id": "Hefcj_nxdOTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377",
      "metadata": {
        "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377"
      },
      "source": [
        "## Train/test model methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee",
      "metadata": {
        "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    running_loss, running_coarse, running_fine = 0.0, 0.0, 0.0\n",
        "    running_consistency = 0.0\n",
        "    correct_coarse, correct_fine = 0, 0\n",
        "    total = 0\n",
        "\n",
        "    for img, labels, ids in dataloader:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        coarse_logits, fine_logits = model(img)\n",
        "\n",
        "        loss, coarse_loss, fine_loss, consistency = criterion(coarse_logits, fine_logits, labels, device)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_coarse += coarse_loss.item()\n",
        "        running_fine += fine_loss.item()\n",
        "        running_consistency += consistency\n",
        "\n",
        "        _, predicted_fine = fine_logits.max(1)\n",
        "        _, predicted_coarse = coarse_logits.max(1)\n",
        "\n",
        "        coarse_labels = criterion.mapping_tensor[labels]\n",
        "\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct_fine += predicted_fine.eq(labels).sum().item()\n",
        "        correct_coarse += predicted_coarse.eq(coarse_labels).sum().item()\n",
        "\n",
        "\n",
        "        print(\".\", end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_coarse = running_coarse / len(dataloader)\n",
        "    epoch_fine = running_fine / len(dataloader)\n",
        "    epoch_consistency = running_consistency / len(dataloader)\n",
        "\n",
        "    epoch_acc_fine = 100.0 * correct_fine / total\n",
        "    epoch_acc_coarse = 100.0 * correct_coarse / total\n",
        "\n",
        "    return {\n",
        "        'total_loss': epoch_loss,\n",
        "        'coarse_loss': epoch_coarse,\n",
        "        'fine_loss': epoch_fine,\n",
        "        'consistency': epoch_consistency,\n",
        "        'fine_acc': epoch_acc_fine,\n",
        "        'coarse_acc': epoch_acc_coarse\n",
        "    }\n",
        "\n",
        "def valid(model, dataloader, device, loss_fn, weighted='F'):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    correct_fine, correct_coarse = 0, 0\n",
        "\n",
        "    all_preds_fine = []\n",
        "    all_preds_coarse = []\n",
        "    all_labels_fine = []\n",
        "    all_labels_coarse = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y, _ in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "\n",
        "            coarse_logits, fine_logits = model(X)\n",
        "\n",
        "            loss, _, _, _ = loss_fn(coarse_logits, fine_logits, y, device)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            predicted_coarse = coarse_logits.argmax(1)\n",
        "\n",
        "            if weighted == 'S':\n",
        "                coarse_probs = F.softmax(coarse_logits, dim=1)\n",
        "                fine_probs = F.softmax(fine_logits, dim=1)\n",
        "\n",
        "                predicted_fine = weighted_fine_simple(fine_probs, coarse_probs, loss_fn.fine_to_coarse_mapping)\n",
        "\n",
        "            elif weighted == 'H':\n",
        "                coarse_probs = F.softmax(coarse_logits, dim=1)\n",
        "                fine_probs = F.softmax(fine_logits, dim=1)\n",
        "                predicted_fine = weighted_fine_HD(fine_probs, coarse_probs, loss_fn.fine_to_coarse_mapping)\n",
        "\n",
        "            else:\n",
        "              predicted_fine = fine_logits.argmax(1)\n",
        "\n",
        "            coarse_labels = loss_fn.mapping_tensor[y]\n",
        "\n",
        "            correct_fine += (predicted_fine == y).type(torch.float).sum().item()\n",
        "            correct_coarse += (predicted_coarse == coarse_labels).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds_fine.extend(predicted_fine.cpu().numpy())\n",
        "            all_preds_coarse.extend(predicted_coarse.cpu().numpy())\n",
        "            all_labels_fine.extend(y.cpu().numpy())\n",
        "            all_labels_coarse.extend(coarse_labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct_fine /= size\n",
        "    correct_coarse /= size\n",
        "\n",
        "    fine_report = classification_report(all_labels_fine, all_preds_fine, digits=4, output_dict=True, zero_division=0)\n",
        "    coarse_report = classification_report(all_labels_coarse, all_preds_coarse, digits=4, output_dict=True, zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'total_loss': test_loss,\n",
        "        'fine_acc': correct_fine * 100,\n",
        "        'coarse_acc': correct_coarse * 100,\n",
        "        'fine_F1': fine_report['macro avg']['f1-score'] * 100,\n",
        "        'coarse_F1': coarse_report['macro avg']['f1-score'] * 100,\n",
        "        'predictions': {\n",
        "            'fine': all_preds_fine,\n",
        "            'coarse': all_preds_coarse,\n",
        "            'labels_fine': all_labels_fine,\n",
        "            'labels_coarse': all_labels_coarse\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def test(model, dataloader, device, loss_fn, weighted='F'):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    correct_fine, correct_coarse = 0, 0\n",
        "\n",
        "    all_preds_fine = []\n",
        "    all_preds_coarse = []\n",
        "    all_labels_fine = []\n",
        "    all_labels_coarse = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y, _ in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "\n",
        "            coarse_logits, fine_logits = model(X)\n",
        "\n",
        "            loss, _, _, _ = loss_fn(coarse_logits, fine_logits, y, device)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            predicted_coarse = coarse_logits.argmax(1)\n",
        "\n",
        "            if weighted == 'S':\n",
        "                coarse_probs = F.softmax(coarse_logits, dim=1)\n",
        "                fine_probs = F.softmax(fine_logits, dim=1)\n",
        "\n",
        "                predicted_fine = weighted_fine_simple(fine_probs, coarse_probs, loss_fn.fine_to_coarse_mapping)\n",
        "\n",
        "            elif weighted == 'H':\n",
        "                coarse_probs = F.softmax(coarse_logits, dim=1)\n",
        "                fine_probs = F.softmax(fine_logits, dim=1)\n",
        "                predicted_fine = weighted_fine_HD(fine_probs, coarse_probs, loss_fn.fine_to_coarse_mapping)\n",
        "\n",
        "            else:\n",
        "              predicted_fine = fine_logits.argmax(1)\n",
        "\n",
        "            coarse_labels = loss_fn.mapping_tensor[y]\n",
        "\n",
        "            correct_fine += (predicted_fine == y).type(torch.float).sum().item()\n",
        "            correct_coarse += (predicted_coarse == coarse_labels).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds_fine.extend(predicted_fine.cpu().numpy())\n",
        "            all_preds_coarse.extend(predicted_coarse.cpu().numpy())\n",
        "            all_labels_fine.extend(y.cpu().numpy())\n",
        "            all_labels_coarse.extend(coarse_labels.cpu().numpy())\n",
        "\n",
        "    all_preds_fine = np.array(all_preds_fine)\n",
        "    all_preds_coarse = np.array(all_preds_coarse)\n",
        "    all_labels_fine = np.array(all_labels_fine)\n",
        "    all_labels_coarse = np.array(all_labels_coarse)\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct_fine /= size\n",
        "    correct_coarse /=size\n",
        "\n",
        "    print(\"Fine classes\")\n",
        "    print(classification_report(all_labels_fine, all_preds_fine, digits=4))\n",
        "\n",
        "    print(\"Coarse classes\")\n",
        "    print(classification_report(all_labels_coarse, all_preds_coarse, digits=4))\n",
        "\n",
        "    true_hierarchical = []\n",
        "    pred_hierarchical = []\n",
        "\n",
        "    for i in range(len(all_labels_fine)):\n",
        "      true_fine = all_labels_fine[i]\n",
        "      true_coarse = all_labels_coarse[i]\n",
        "\n",
        "      true_hierarchical.append([true_coarse, true_fine])\n",
        "\n",
        "      pred_fine = all_preds_fine[i]\n",
        "      pred_coarse = all_preds_coarse[i]\n",
        "\n",
        "      pred_hierarchical.append([pred_coarse, pred_fine])\n",
        "\n",
        "    num_coarse = coarse_logits.shape[1]\n",
        "    num_fine = fine_logits.shape[1]\n",
        "\n",
        "    h_f1_per_class = hierarchical_f1_per_class(\n",
        "        true_hierarchical,\n",
        "        pred_hierarchical,\n",
        "        num_coarse,\n",
        "        num_fine\n",
        "    )\n",
        "\n",
        "    print(\"Hierarchical F1\")\n",
        "    for cls, score in h_f1_per_class.items():\n",
        "        print(f\"Class coarse={cls[0]}, fine={cls[1]}: {score * 100:.2f}\")\n",
        "\n",
        "\n",
        "    h_f1 = hf1(true_hierarchical, pred_hierarchical)\n",
        "    print(f\"Hierarchical Macro Avg F1: {h_f1 * 100:.2f}\")\n",
        "\n",
        "    return {\n",
        "        'total_loss': test_loss,\n",
        "        'fine_acc': correct_fine * 100,\n",
        "        'coarse_acc': correct_coarse * 100\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted fine predictions"
      ],
      "metadata": {
        "id": "uM-HClVKPe-G"
      },
      "id": "uM-HClVKPe-G"
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_fine_simple(fine_probs, coarse_probs, fine_to_coarse_mapping):\n",
        "  weighted_probs = fine_probs.clone()\n",
        "\n",
        "  for fine_id, coarse_id in fine_to_coarse_mapping.items():\n",
        "    weighted_probs[:, fine_id] *= coarse_probs[:, coarse_id]\n",
        "\n",
        "  predicted_fine = weighted_probs.argmax(dim=1)\n",
        "\n",
        "  return predicted_fine\n",
        "\n",
        "\n",
        "def weighted_fine_HD(fine_probs, coarse_probs, fine_to_coarse_mapping):\n",
        "  batch_size = fine_probs.shape[0]\n",
        "  num_fine_classes = fine_probs.shape[1]\n",
        "  # Corrected calculation for num_coarse_classes\n",
        "  num_coarse_classes = max(fine_to_coarse_mapping.values()) + 1\n",
        "  device = fine_probs.device\n",
        "\n",
        "\n",
        "  coarse_to_fine = {}\n",
        "\n",
        "  for fine_id, coarse_id in fine_to_coarse_mapping.items():\n",
        "    if coarse_id not in coarse_to_fine:\n",
        "      coarse_to_fine[coarse_id] = []\n",
        "    coarse_to_fine[coarse_id].append(fine_id)\n",
        "\n",
        "  weighted_probs = torch.zeros_like(fine_probs)\n",
        "\n",
        "  for coarse_id in range(num_coarse_classes):\n",
        "    fine_ids = coarse_to_fine.get(coarse_id, [])\n",
        "\n",
        "    if len(fine_ids) == 0:\n",
        "      continue\n",
        "\n",
        "\n",
        "    fine_ids_tensor = torch.tensor(fine_ids, device=device)\n",
        "    fine_probs_coarse = fine_probs[:, fine_ids_tensor]\n",
        "\n",
        "    fine_probs_sum = fine_probs_coarse.sum(dim=1, keepdim=True) + 1e-10 # so no /0\n",
        "    cond_probs = fine_probs_coarse / fine_probs_sum\n",
        "\n",
        "    coarse_prob = coarse_probs[:, coarse_id].unsqueeze(1)\n",
        "    weighted_fine_probs = cond_probs * coarse_prob\n",
        "\n",
        "    weighted_probs[:, fine_ids_tensor] = weighted_fine_probs\n",
        "\n",
        "  predicted_fine = weighted_probs.argmax(dim=1)\n",
        "\n",
        "  return predicted_fine"
      ],
      "metadata": {
        "id": "hyIA8i0iPmNJ"
      },
      "id": "hyIA8i0iPmNJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "sRDJ8Ph-vuwQ"
      },
      "id": "sRDJ8Ph-vuwQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7264957-8f92-4177-d6b2-a78dc4825429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 71.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "fine_to_coarse = {\n",
        "    0:0, 1:0, 2:0,\n",
        "    3:1,\n",
        "    4:2, 5:2, 6:2, 7:2,\n",
        "    8:3\n",
        "}\n",
        "\n",
        "num_coarse = 4\n",
        "num_fine = 9\n",
        "\n",
        "model = HierarchicalResNet(num_coarse, num_fine, freeze_backbone=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "fine_weights, coarse_weights = calculate_class_weights(train_df[label_col], fine_to_coarse, device)\n",
        "\n",
        "train_criterion = HierarchicalLoss(fine_to_coarse, alpha=0.5, beta=0.3, fine_weights=fine_weights, coarse_weights=coarse_weights)\n",
        "test_criterion = HierarchicalLoss(fine_to_coarse, alpha=0.5, beta=0.3)\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
        "\n",
        "best_acc = 0.0\n",
        "\n",
        "epochs = 0\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    train_metrics = train_one_epoch(model, train_loader, train_criterion, optimizer, device)\n",
        "    print(f\"Train - Loss: {train_metrics['total_loss']:.4f}, \"\n",
        "          f\"Fine Acc: {train_metrics['fine_acc']:.2f}%, \"\n",
        "          f\"Coarse Acc: {train_metrics['coarse_acc']:.2f}\")\n",
        "\n",
        "    valid_metrics = valid(model, val_loader, device, test_criterion)\n",
        "    print(f\"Valid - Loss: {valid_metrics['total_loss']:.4f}, \"\n",
        "          f\"Fine Acc: {valid_metrics['fine_acc']:.2f}% | Fine F1: {valid_metrics['fine_F1']:.2f}%, \"\n",
        "          f\"Coarse Acc: {valid_metrics['coarse_acc']:.2f}% | Coarse F1: {valid_metrics['coarse_F1']:.2f}%\")\n",
        "\n",
        "    scheduler.step(valid_metrics['total_loss'])\n",
        "\n",
        "    if valid_metrics['fine_F1'] > best_acc:\n",
        "        best_acc = valid_metrics['fine_F1']\n",
        "        torch.save(model.state_dict(), path + 'layer.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model"
      ],
      "metadata": {
        "id": "Ci11a7Nkv5ZZ"
      },
      "id": "Ci11a7Nkv5ZZ"
    },
    {
      "cell_type": "code",
      "source": [
        "model = HierarchicalResNet(num_coarse, num_fine, freeze_backbone=False)\n",
        "\n",
        "model.load_state_dict(torch.load(path + 'layer_base.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_metrics = test(model, test_loader, device, test_criterion, weighted='F')\n",
        "test_metrics_norm = test(model, test_loader, device, test_criterion, weighted='S')\n",
        "test_metrics_weights = test(model, test_loader, device, test_criterion, weighted='H')\n",
        "\n"
      ],
      "metadata": {
        "id": "KhNNyfh5v8Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1ddab6-f435-4d11-d8aa-2df78868f4e7"
      },
      "id": "KhNNyfh5v8Yn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4286    0.7500    0.5455         4\n",
            "           1     0.8444    0.8444    0.8444        45\n",
            "           2     0.4667    0.7778    0.5833         9\n",
            "           3     0.8953    0.7196    0.7979       107\n",
            "           4     0.6803    0.7407    0.7092       135\n",
            "           5     0.7432    0.6869    0.7139       198\n",
            "           6     0.6707    0.7333    0.7006       150\n",
            "           7     0.8380    0.8287    0.8333       181\n",
            "           8     0.7879    0.8254    0.8062        63\n",
            "\n",
            "    accuracy                         0.7545       892\n",
            "   macro avg     0.7061    0.7674    0.7261       892\n",
            "weighted avg     0.7630    0.7545    0.7563       892\n",
            "\n",
            "Coarse classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8361    0.8793    0.8571        58\n",
            "           1     0.7287    0.8785    0.7966       107\n",
            "           2     0.9871    0.9247    0.9549       664\n",
            "           3     0.7125    0.9048    0.7972        63\n",
            "\n",
            "    accuracy                         0.9148       892\n",
            "   macro avg     0.8161    0.8968    0.8515       892\n",
            "weighted avg     0.9269    0.9148    0.9184       892\n",
            "\n",
            "Hierarchical F1\n",
            "Class coarse=2, fine=7: 85.64\n",
            "Class coarse=3, fine=8: 86.51\n",
            "Class coarse=2, fine=4: 77.04\n",
            "Class coarse=2, fine=5: 84.09\n",
            "Class coarse=2, fine=6: 86.33\n",
            "Class coarse=1, fine=3: 82.71\n",
            "Class coarse=0, fine=1: 91.53\n",
            "Class coarse=0, fine=2: 83.33\n",
            "Class coarse=0, fine=0: 66.67\n",
            "Hierarchical Macro Avg F1: 84.04\n",
            "Fine classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.7500    0.6000         4\n",
            "           1     0.8409    0.8222    0.8315        45\n",
            "           2     0.4667    0.7778    0.5833         9\n",
            "           3     0.8131    0.8131    0.8131       107\n",
            "           4     0.7266    0.6889    0.7072       135\n",
            "           5     0.7432    0.6869    0.7139       198\n",
            "           6     0.6707    0.7333    0.7006       150\n",
            "           7     0.8547    0.8122    0.8329       181\n",
            "           8     0.7671    0.8889    0.8235        63\n",
            "\n",
            "    accuracy                         0.7578       892\n",
            "   macro avg     0.7092    0.7748    0.7340       892\n",
            "weighted avg     0.7622    0.7578    0.7585       892\n",
            "\n",
            "Coarse classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8361    0.8793    0.8571        58\n",
            "           1     0.7287    0.8785    0.7966       107\n",
            "           2     0.9871    0.9247    0.9549       664\n",
            "           3     0.7125    0.9048    0.7972        63\n",
            "\n",
            "    accuracy                         0.9148       892\n",
            "   macro avg     0.8161    0.8968    0.8515       892\n",
            "weighted avg     0.9269    0.9148    0.9184       892\n",
            "\n",
            "Hierarchical F1\n",
            "Class coarse=2, fine=7: 84.81\n",
            "Class coarse=3, fine=8: 89.68\n",
            "Class coarse=2, fine=4: 74.44\n",
            "Class coarse=2, fine=5: 84.09\n",
            "Class coarse=2, fine=6: 86.33\n",
            "Class coarse=1, fine=3: 87.38\n",
            "Class coarse=0, fine=1: 91.01\n",
            "Class coarse=0, fine=2: 83.33\n",
            "Class coarse=0, fine=0: 66.67\n",
            "Hierarchical Macro Avg F1: 84.24\n",
            "Fine classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7500    0.7500    0.7500         4\n",
            "           1     0.8333    0.7778    0.8046        45\n",
            "           2     0.4615    0.6667    0.5455         9\n",
            "           3     0.7143    0.8879    0.7917       107\n",
            "           4     0.7315    0.5852    0.6502       135\n",
            "           5     0.7418    0.6818    0.7105       198\n",
            "           6     0.6707    0.7333    0.7006       150\n",
            "           7     0.8537    0.7735    0.8116       181\n",
            "           8     0.6951    0.9048    0.7862        63\n",
            "\n",
            "    accuracy                         0.7399       892\n",
            "   macro avg     0.7169    0.7512    0.7279       892\n",
            "weighted avg     0.7462    0.7399    0.7386       892\n",
            "\n",
            "Coarse classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8361    0.8793    0.8571        58\n",
            "           1     0.7287    0.8785    0.7966       107\n",
            "           2     0.9871    0.9247    0.9549       664\n",
            "           3     0.7125    0.9048    0.7972        63\n",
            "\n",
            "    accuracy                         0.9148       892\n",
            "   macro avg     0.8161    0.8968    0.8515       892\n",
            "weighted avg     0.9269    0.9148    0.9184       892\n",
            "\n",
            "Hierarchical F1\n",
            "Class coarse=2, fine=7: 82.87\n",
            "Class coarse=3, fine=8: 90.48\n",
            "Class coarse=2, fine=4: 69.26\n",
            "Class coarse=2, fine=5: 83.84\n",
            "Class coarse=2, fine=6: 86.33\n",
            "Class coarse=1, fine=3: 91.12\n",
            "Class coarse=0, fine=1: 89.39\n",
            "Class coarse=0, fine=2: 77.78\n",
            "Class coarse=0, fine=0: 66.67\n",
            "Hierarchical Macro Avg F1: 83.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "new Audio('https://actions.google.com/sounds/v1/alarms/beep_short.ogg').play();"
      ],
      "metadata": {
        "id": "_ZYM_fIrz2ph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3974cccb-64f0-413f-985f-fb980353075d"
      },
      "id": "_ZYM_fIrz2ph",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "new Audio('https://actions.google.com/sounds/v1/alarms/beep_short.ogg').play();\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}