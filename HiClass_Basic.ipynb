{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac",
      "metadata": {
        "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac"
      },
      "source": [
        "## Base Line Model\n",
        "with Local Binary Pattern, Mobile Net 3 small with 1 added layer, flat architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6",
      "metadata": {
        "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
      "metadata": {
        "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5138997-819b-45f7-b2dc-ae445cc7f5c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hiclass[all]\n",
            "  Downloading hiclass-5.0.4-py3-none-any.whl.metadata (16 kB)\n",
            "\u001b[33mWARNING: hiclass 5.0.4 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.5 in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5->hiclass[all]) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5->hiclass[all]) (3.6.0)\n",
            "Downloading hiclass-5.0.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hiclass\n",
            "Successfully installed hiclass-5.0.4\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install hiclass[all]\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import color\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from hiclass import LocalClassifierPerNode, LocalClassifierPerParentNode, LocalClassifierPerLevel\n",
        "from hiclass.metrics import f1\n",
        "import sklearn\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044c180a-d6d9-4bf8-9545-4f4147733a61",
      "metadata": {
        "id": "044c180a-d6d9-4bf8-9545-4f4147733a61"
      },
      "source": [
        "### LBP Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53000783-494e-44d7-88c7-0900c1fdfafc",
      "metadata": {
        "id": "53000783-494e-44d7-88c7-0900c1fdfafc"
      },
      "outputs": [],
      "source": [
        "class LBPTransform:\n",
        "    def __init__(self, radius=3, n_points=None, method='uniform'):\n",
        "        self.radius = radius\n",
        "        self.n_points = n_points if n_points else 8 * radius\n",
        "        self.method = method\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        if len(img.shape) == 3 :\n",
        "            gray = color.rgb2gray(img)\n",
        "        else:\n",
        "            gray = img\n",
        "\n",
        "        gray = (gray * 255).astype(np.uint8)\n",
        "\n",
        "        lbp = local_binary_pattern(gray, self.n_points, self.radius, self.method)\n",
        "\n",
        "        lbp = (lbp - lbp.min()) / (lbp.max() - lbp.min() + 1e-7)\n",
        "\n",
        "        lbp_3 = np.stack([lbp, lbp, lbp], axis=-1)\n",
        "\n",
        "        return lbp_3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1",
      "metadata": {
        "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1"
      },
      "source": [
        "### Make LBP Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1",
      "metadata": {
        "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1"
      },
      "outputs": [],
      "source": [
        "def make_lbp_csv(input_folder, csv_path, lbp_transformer):\n",
        "    img_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    print(f\"Processing {len(img_files)} images from {input_folder}...\")\n",
        "    with open(csv_path, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        header_written=False\n",
        "\n",
        "        for img, fname in enumerate(img_files):\n",
        "          in_path = os.path.join(input_folder, fname)\n",
        "          img = Image.open(in_path).convert('RGB')\n",
        "\n",
        "          img = lbp_transformer(img)\n",
        "          img = img.flatten()\n",
        "\n",
        "          if not header_written:\n",
        "            header = ['PGCname'] + [f'pixel_{i}' for i in range(len(img))]\n",
        "            writer.writerow(header)\n",
        "            header_written = True\n",
        "\n",
        "          writer.writerow([fname] + img.tolist)\n",
        "\n",
        "    print(\"LBP preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3",
      "metadata": {
        "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3"
      },
      "source": [
        "### Dataset Class\n",
        "Class for processing data and combining images with labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479",
      "metadata": {
        "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479"
      },
      "outputs": [],
      "source": [
        "class PGCDataset(Dataset):\n",
        "    def __init__(self, labels_df, img_folder, id_col='PGCname', label_col='T', transform=None):\n",
        "        self.labels_df = labels_df.reset_index(drop=True)\n",
        "        self.img_folder = img_folder\n",
        "        self.id_col = id_col\n",
        "        self.label_col = label_col\n",
        "        self.transform = transform\n",
        "\n",
        "        available_imgs = {f.replace('.png', '') for f in os.listdir(img_folder)\n",
        "                            if f.endswith('.png')}\n",
        "        self.labels_df = self.labels_df[self.labels_df[id_col].isin(available_imgs)].reset_index(drop=True)\n",
        "\n",
        "        print(f\"Dataset created with {len(self.labels_df)} imgs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.labels_df.iloc[idx]\n",
        "\n",
        "        img_id = row[self.id_col]\n",
        "        img_path = os.path.join(self.img_folder, f\"{img_id}.png\")\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        label = torch.tensor(int(row[self.label_col]), dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label, img_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5",
      "metadata": {
        "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
        "outputId": "550697fd-c0ab-4178-fc13-96a61882a313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 3120 imgs\n",
            "Dataset created with 446 imgs\n",
            "Dataset created with 892 imgs\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/Othercomputers/My laptop/Thesis/Galaxy-Classifier/'\n",
        "img_folder = path + 'images'\n",
        "lbp_csv = path + 'lbp.csv'\n",
        "\n",
        "\n",
        "id_col = 'PGCname'\n",
        "label_col = 'T'\n",
        "\n",
        "img_size = 224\n",
        "\n",
        "labels_df = pd.read_csv(path + 'EFIGI_attributes.txt', sep=r'\\s+', comment='#')\n",
        "\n",
        "labels = labels_df[label_col]\n",
        "\n",
        "labels = labels.replace({-3:-2, -1:-2}) # S0\n",
        "labels = labels.replace({0:1, 2:1}) # Sa\n",
        "labels = labels.replace({3:4}) # Sb\n",
        "labels = labels.replace({5:6}) # Sc\n",
        "labels = labels.replace({8:7, 9:7}) # Sd\n",
        "labels = labels.replace({10:11}) # Irr\n",
        "\n",
        "labels = labels.replace({-6:0, -5:1, -4:2, -2:3, 1:4, 4:5, 11:8}) # Adjust to 0 - 8\n",
        "\n",
        "labels_df[label_col] = labels\n",
        "\n",
        "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=0, stratify=labels_df[label_col])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.125, random_state=0, stratify=train_df[label_col])\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(180),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = PGCDataset(\n",
        "    labels_df=train_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = PGCDataset(\n",
        "    labels_df=val_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")\n",
        "test_dataset = PGCDataset(\n",
        "    labels_df=test_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62",
      "metadata": {
        "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62"
      },
      "source": [
        "### Data loader\n",
        "loads data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26",
      "metadata": {
        "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26"
      },
      "outputs": [],
      "source": [
        "\"\"\"labels = train_df[label_col].values\n",
        "classes= np.unique(labels)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=labels)\"\"\"\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed",
      "metadata": {
        "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed"
      },
      "source": [
        "### Hierarchical model\n",
        "using pretrained resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146c13bc-506b-49d8-8b6e-095e8e170259",
      "metadata": {
        "id": "146c13bc-506b-49d8-8b6e-095e8e170259"
      },
      "outputs": [],
      "source": [
        "class FeatureResnet18(nn.Module):\n",
        "  def __init__(self, num_classes=None):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    # get model\n",
        "    resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "    self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "    self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    features = self.backbone(x)\n",
        "    features = torch.flatten(features, 1)\n",
        "\n",
        "    return features, self.fc(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate class weights"
      ],
      "metadata": {
        "id": "xd3-ttA8KBK3"
      },
      "id": "xd3-ttA8KBK3"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(labels, fine_per_coarse, device='cpu'):\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    fine_to_coarse_mapping = {}\n",
        "    fine_id = 0\n",
        "    for coarse_id, num_fine in enumerate(fine_per_coarse):\n",
        "      for local_fine_id in range(num_fine):\n",
        "        fine_to_coarse_mapping[fine_id] = coarse_id\n",
        "        fine_id += 1\n",
        "\n",
        "    fine_classes = np.unique(labels)\n",
        "    fine_weights = compute_class_weight('balanced', classes=fine_classes, y=labels)\n",
        "\n",
        "    total_fine_classes = sum(fine_per_coarse)\n",
        "    fine_weights_full = torch.ones(total_fine_classes, dtype=torch.float, device=device)\n",
        "    for i, class_id in enumerate(fine_classes):\n",
        "        fine_weights_full[class_id] = fine_weights[i]\n",
        "\n",
        "    coarse_labels = np.array([fine_to_coarse_mapping[label] for label in labels])\n",
        "    coarse_classes = np.unique(coarse_labels)\n",
        "    coarse_weights = compute_class_weight('balanced', classes=coarse_classes, y=coarse_labels)\n",
        "\n",
        "    num_coarse_classes = len(fine_per_coarse)\n",
        "    coarse_weights_full = torch.ones(num_coarse_classes, dtype=torch.float, device=device)\n",
        "    for i, class_id in enumerate(coarse_classes):\n",
        "        coarse_weights_full[class_id] = coarse_weights[i]\n",
        "\n",
        "    return fine_weights_full, coarse_weights_full"
      ],
      "metadata": {
        "id": "B0JYvXK2KHZr"
      },
      "id": "B0JYvXK2KHZr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377",
      "metadata": {
        "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377"
      },
      "source": [
        "## Train/extract feature methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee",
      "metadata": {
        "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee"
      },
      "outputs": [],
      "source": [
        "def train_CNN(model, dataloader, criterion, optimizer, device, epochs):\n",
        "    model.train()\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      running_loss, correct, total = 0.0, 0, 0\n",
        "      for img, labels, ids in dataloader:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, outputs = model(img)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        print(\".\", end=\"\")\n",
        "      print(\"\")\n",
        "\n",
        "      epoch_loss = running_loss / len(dataloader)\n",
        "      epoch_acc = 100.0 * correct / total\n",
        "      print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "      print(f\"  Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.2f}%\")\n",
        "\n",
        "      if epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        torch.save(model.state_dict(), path + 'feature_CNN.pth')\n",
        "\n",
        "\n",
        "def extract_features(model, dataloader, device, file_name=None, loss_fn=None):\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "  test_loss, acc = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y, _ in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      features, outputs = model(X)\n",
        "\n",
        "      if loss_fn:\n",
        "        loss = loss_fn(outputs, y)\n",
        "        test_loss += loss.item()\n",
        "        acc += (outputs.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        all_preds.append(outputs.argmax(1).cpu().numpy())\n",
        "\n",
        "      all_features.append(features.cpu().numpy())\n",
        "      all_labels.append(y.cpu().numpy())\n",
        "\n",
        "      print(\".\", end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "  X = np.vstack(all_features)\n",
        "  y = np.concatenate(all_labels)\n",
        "\n",
        "  if loss_fn:\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "\n",
        "    print(classification_report(y, all_preds, digits=4))\n",
        "\n",
        "  if file_name:\n",
        "    np.save(path + file_name + '_X.npy', X)\n",
        "    np.save(path + file_name + '_y.npy', y)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def convert_labels(labels, fine_to_coarse):\n",
        "  hierarchical_labels = []\n",
        "\n",
        "  for fine in labels:\n",
        "    coarse = fine_to_coarse[fine]\n",
        "    hierarchical_labels.append([coarse, fine])\n",
        "\n",
        "  return hierarchical_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train feature model and extract features"
      ],
      "metadata": {
        "id": "sRDJ8Ph-vuwQ"
      },
      "id": "sRDJ8Ph-vuwQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
        "outputId": "a84a569c-6ba0-4314-d8d3-931c8cd3b80e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 47.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.1579    0.7500    0.2609         4\n",
            "           1     0.7209    0.6889    0.7045        45\n",
            "           2     0.8571    0.6667    0.7500         9\n",
            "           3     0.7059    0.5607    0.6250       107\n",
            "           4     0.6000    0.6667    0.6316       135\n",
            "           5     0.6961    0.7172    0.7065       198\n",
            "           6     0.7143    0.5333    0.6107       150\n",
            "           7     0.7415    0.8398    0.7876       181\n",
            "           8     0.7313    0.7778    0.7538        63\n",
            "\n",
            "    accuracy                         0.6872       892\n",
            "   macro avg     0.6583    0.6890    0.6478       892\n",
            "weighted avg     0.6979    0.6872    0.6874       892\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"feature_name = 'feature_CNN.pth'\n",
        "\n",
        "fine_to_coarse = {\n",
        "    0:0, 1:0, 2:0,\n",
        "    3:1,\n",
        "    4:2, 5:2, 6:2, 7:2,\n",
        "    8:3\n",
        "}\n",
        "\n",
        "num_classes = 9\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "feature_CNN = FeatureResnet18(num_classes).to(device)\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_df[label_col]), y=train_df[label_col])\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(class_weights)\n",
        "test_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(feature_CNN.parameters(), lr=0.001)\n",
        "\n",
        "#train_CNN(feature_CNN, train_loader, criterion, optimizer, device, epochs=50)\n",
        "\n",
        "feature_CNN.load_state_dict(torch.load(path + feature_name))\n",
        "\n",
        "#X_train, y_train = extract_features(feature_CNN, train_loader, device, file_name=\"train\")\n",
        "X_test, y_test = extract_features(feature_CNN, test_loader, device, file_name=\"test\", loss_fn=test_criterion)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test classifier"
      ],
      "metadata": {
        "id": "Ci11a7Nkv5ZZ"
      },
      "id": "Ci11a7Nkv5ZZ"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"model_name = 'Hi_node.joblib'\n",
        "\n",
        "X_train = np.load(path + 'train_X.npy')\n",
        "y_train_f = np.load(path + 'train_y.npy')\n",
        "\n",
        "X_test = np.load(path + 'test_X.npy')\n",
        "y_test_f = np.load(path + 'test_y.npy')\n",
        "\n",
        "y_train_h = convert_labels(y_train_f, fine_to_coarse)\n",
        "y_test_h = convert_labels(y_test_f, fine_to_coarse)\n",
        "\n",
        "local_classifier = sklearn.svm.SVC(C=5, kernel=\"rbf\", gamma=\"scale\", probability=True, class_weight=\"balanced\")\n",
        "\n",
        "classifier = LocalClassifierPerNode(local_classifier, n_jobs=-1, verbose=20)\n",
        "\n",
        "classifier.fit(X_train, y_train_h)\n",
        "\n",
        "joblib.dump(classifier, path + model_name)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "KhNNyfh5v8Yn",
        "outputId": "7cb556ea-39d6-4639-fdee-be651cb72482"
      },
      "id": "KhNNyfh5v8Yn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_name = \\'Hi_node.joblib\\'\\n\\nX_train = np.load(path + \\'train_X.npy\\')\\ny_train_f = np.load(path + \\'train_y.npy\\')\\n\\nX_test = np.load(path + \\'test_X.npy\\')\\ny_test_f = np.load(path + \\'test_y.npy\\')\\n\\ny_train_h = convert_labels(y_train_f, fine_to_coarse)\\ny_test_h = convert_labels(y_test_f, fine_to_coarse)\\n\\nlocal_classifier = sklearn.svm.SVC(C=5, kernel=\"rbf\", gamma=\"scale\", probability=True, class_weight=\"balanced\")\\n\\nclassifier = LocalClassifierPerNode(local_classifier, n_jobs=-1, verbose=20)\\n\\nclassifier.fit(X_train, y_train_h)\\n\\njoblib.dump(classifier, path + model_name)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = joblib.load(path + model_name)\n",
        "\n",
        "y_pred_h = classifier.predict(X_test)\n",
        "\n",
        "y_pred_f = np.array([int(pred[-1]) for pred in y_pred_h])\n",
        "\n",
        "\n",
        "print(classification_report(y_test_f, y_pred_f))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lx1V1FZ9rLSz",
        "outputId": "b71cc533-1780-4f32-942c-4e00297bc954"
      },
      "id": "lx1V1FZ9rLSz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#classifier = joblib.load(path + model_name)\\n\\ny_pred_h = classifier.predict(X_test)\\n\\ny_pred_f = np.array([int(pred[-1]) for pred in y_pred_h])\\n\\n\\nprint(classification_report(y_test_f, y_pred_f))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}