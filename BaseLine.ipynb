{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac",
      "metadata": {
        "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac"
      },
      "source": [
        "## Base Line Model\n",
        "with Local Binary Pattern, resnet18 small with 1 added layer, flat architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6",
      "metadata": {
        "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
        "outputId": "0b1c77f5-be2c-4097-9b74-3ab4ee03ec03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import color\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044c180a-d6d9-4bf8-9545-4f4147733a61",
      "metadata": {
        "id": "044c180a-d6d9-4bf8-9545-4f4147733a61"
      },
      "source": [
        "### LBP Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53000783-494e-44d7-88c7-0900c1fdfafc",
      "metadata": {
        "id": "53000783-494e-44d7-88c7-0900c1fdfafc"
      },
      "outputs": [],
      "source": [
        "class LBPTransform:\n",
        "    def __init__(self, radius=3, n_points=None, method='uniform'):\n",
        "        self.radius = radius\n",
        "        self.n_points = n_points if n_points else 8 * radius\n",
        "        self.method = method\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        if len(img.shape) == 3 :\n",
        "            gray = color.rgb2gray(img)\n",
        "        else:\n",
        "            gray = img\n",
        "\n",
        "        gray = (gray * 255).astype(np.uint8)\n",
        "\n",
        "        lbp = local_binary_pattern(gray, self.n_points, self.radius, self.method)\n",
        "\n",
        "        lbp = (lbp - lbp.min()) / (lbp.max() - lbp.min() + 1e-7)\n",
        "\n",
        "        lbp_3 = np.stack([lbp, lbp, lbp], axis=-1)\n",
        "\n",
        "        return lbp_3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1",
      "metadata": {
        "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1"
      },
      "source": [
        "### Make LBP Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1",
      "metadata": {
        "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1"
      },
      "outputs": [],
      "source": [
        "def make_lbp_csv(input_folder, csv_path, lbp_transformer):\n",
        "    img_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    print(f\"Processing {len(img_files)} images from {input_folder}...\")\n",
        "    with open(csv_path, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        header_written=False\n",
        "\n",
        "        for img, fname in enumerate(img_files):\n",
        "          in_path = os.path.join(input_folder, fname)\n",
        "          img = Image.open(in_path).convert('RGB')\n",
        "\n",
        "          img = lbp_transformer(img)\n",
        "          img = img.flatten()\n",
        "\n",
        "          if not header_written:\n",
        "            header = ['PGCname'] + [f'pixel_{i}' for i in range(len(img))]\n",
        "            writer.writerow(header)\n",
        "            header_written = True\n",
        "\n",
        "          writer.writerow([fname] + img.tolist)\n",
        "\n",
        "    print(\"LBP preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3",
      "metadata": {
        "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3"
      },
      "source": [
        "### Dataset Class\n",
        "Class for processing data and combining images with labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479",
      "metadata": {
        "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479"
      },
      "outputs": [],
      "source": [
        "class PGCDataset(Dataset):\n",
        "    def __init__(self, labels_df, img_folder, id_col='PGCname', label_col='T', transform=None):\n",
        "        self.labels_df = labels_df.reset_index(drop=True)\n",
        "        self.img_folder = img_folder\n",
        "        self.id_col = id_col\n",
        "        self.label_col = label_col\n",
        "        self.transform = transform\n",
        "\n",
        "        available_imgs = {f.replace('.png', '') for f in os.listdir(img_folder)\n",
        "                            if f.endswith('.png')}\n",
        "        self.labels_df = self.labels_df[self.labels_df[id_col].isin(available_imgs)].reset_index(drop=True)\n",
        "\n",
        "        print(f\"Dataset created with {len(self.labels_df)} imgs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.labels_df.iloc[idx]\n",
        "\n",
        "        img_id = row[self.id_col]\n",
        "        img_path = os.path.join(self.img_folder, f\"{img_id}.png\")\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        label = torch.tensor(int(row[self.label_col]), dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label, img_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5",
      "metadata": {
        "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
        "outputId": "0a342a6f-d03d-4b0f-c8da-b72fc7fc6107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 3120 imgs\n",
            "Dataset created with 446 imgs\n",
            "Dataset created with 892 imgs\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/Othercomputers/My laptop/Thesis/Galaxy-Classifier/'\n",
        "img_folder = path + '/images'\n",
        "lbp_img_folder = path + '/lbp_images'\n",
        "\n",
        "id_col = 'PGCname'\n",
        "label_col = 'T'\n",
        "\n",
        "img_size = 224\n",
        "\n",
        "labels_df = pd.read_csv(path + 'EFIGI_attributes.txt', sep=r'\\s+', comment='#')\n",
        "#labels_df[label_col] = labels_df[label_col].replace({-6:-4, -5:-4}) # E\n",
        "labels_df[label_col] = labels_df[label_col].replace({-3:-2, -1:-2}) # S0\n",
        "labels_df[label_col] = labels_df[label_col].replace({0:1, 2:1}) # Sa\n",
        "labels_df[label_col] = labels_df[label_col].replace({3:4}) # Sb\n",
        "labels_df[label_col] = labels_df[label_col].replace({5:6}) # Sc\n",
        "labels_df[label_col] = labels_df[label_col].replace({8:7, 9:7}) # Sd\n",
        "labels_df[label_col] = labels_df[label_col].replace({10:11}) # Irr\n",
        "\n",
        "labels_df[label_col] = labels_df[label_col].replace({-6:0, -5:1, -4:2, -2:3, 1:4, 4:5, 11:8}) # Adjust to 0 - 8\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=0, stratify=labels_df[label_col])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.125, random_state=0, stratify=train_df[label_col])\n",
        "\n",
        "# use stratify sampling in training - write to csv file - - tocsv.pandas\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(180),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\"\"\"lbp_params = {'radius': 3, 'n_points': 24, 'method': 'uniform'}\n",
        "\n",
        "lbp_transform = LBPTransform(**lbp_params)\n",
        "\n",
        "make_lbp_images(img_folder, lbp_img_folder, lbp_transform)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_dataset = PGCDataset(\n",
        "    labels_df=train_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = PGCDataset(\n",
        "    labels_df=val_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")\n",
        "test_dataset = PGCDataset(\n",
        "    labels_df=test_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62",
      "metadata": {
        "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62"
      },
      "source": [
        "### Data loader\n",
        "loads data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26",
      "metadata": {
        "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26"
      },
      "outputs": [],
      "source": [
        "labels = train_df[label_col].values\n",
        "classes= np.unique(labels)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
        "\n",
        "sample_weights = np.array([class_weights[np.where(classes == label)[0][0]] for label in labels])\n",
        "sample_weights = torch.from_numpy(sample_weights).float()\n",
        "\n",
        "sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed",
      "metadata": {
        "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed"
      },
      "source": [
        "### Make model\n",
        "using pretrained resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146c13bc-506b-49d8-8b6e-095e8e170259",
      "metadata": {
        "id": "146c13bc-506b-49d8-8b6e-095e8e170259"
      },
      "outputs": [],
      "source": [
        "def resnet_model(num_classes, freeze_backbone=True):\n",
        "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = not freeze_backbone\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377",
      "metadata": {
        "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377"
      },
      "source": [
        "## Train/test model methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee",
      "metadata": {
        "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for img, labels, ids in dataloader:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(img)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        print(\".\", end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def valid(model, dataloader, device, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y, _ in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True, digits=4, zero_devision=0)\n",
        "\n",
        "    return test_loss, correct * 100, report['macro avg']['f1-score'] * 100\n",
        "\n",
        "\n",
        "def test(model, dataloader, device, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y, _ in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "    return test_loss, correct * 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sRDJ8Ph-vuwQ",
      "metadata": {
        "id": "sRDJ8Ph-vuwQ"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
        "jp-MarkdownHeadingCollapsed": true,
        "outputId": "4ea47436-cb39-47d1-a4c7-b994ae1f19a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            ".................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "  Train Loss: 1.4781 | Train Acc: 46.38%\n",
            "  Val   Loss: 1.5529 | Val   Acc: 42.60 | Val   Macro F1: 0.34059806450650554%\n",
            ".................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50\n",
            "  Train Loss: 1.2302 | Train Acc: 53.37%\n",
            "  Val   Loss: 0.9999 | Val   Acc: 60.31 | Val   Macro F1: 0.448129252203908%\n",
            ".................................................\n",
            "Epoch 3/50\n",
            "  Train Loss: 1.0190 | Train Acc: 58.40%\n",
            "  Val   Loss: 0.9202 | Val   Acc: 63.68 | Val   Macro F1: 0.5781589241290483%\n",
            ".................................................\n",
            "Epoch 4/50\n",
            "  Train Loss: 0.9290 | Train Acc: 61.83%\n",
            "  Val   Loss: 1.6042 | Val   Acc: 43.05 | Val   Macro F1: 0.39344950383355926%\n",
            ".................................................\n",
            "Epoch 5/50\n",
            "  Train Loss: 0.8486 | Train Acc: 64.07%\n",
            "  Val   Loss: 0.8560 | Val   Acc: 63.23 | Val   Macro F1: 0.581138454229038%\n",
            ".................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50\n",
            "  Train Loss: 0.9890 | Train Acc: 58.59%\n",
            "  Val   Loss: 1.3372 | Val   Acc: 47.76 | Val   Macro F1: 0.36994586484333936%\n",
            ".................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50\n",
            "  Train Loss: 0.9382 | Train Acc: 61.57%\n",
            "  Val   Loss: 1.1424 | Val   Acc: 54.71 | Val   Macro F1: 0.4194351965545643%\n",
            ".................................................\n",
            "Epoch 8/50\n",
            "  Train Loss: 0.9755 | Train Acc: 61.06%\n",
            "  Val   Loss: 0.9232 | Val   Acc: 61.88 | Val   Macro F1: 0.5722954039542443%\n",
            ".................................................\n",
            "Epoch 9/50\n",
            "  Train Loss: 0.7498 | Train Acc: 65.54%\n",
            "  Val   Loss: 0.8867 | Val   Acc: 62.78 | Val   Macro F1: 0.515664617649547%\n",
            ".................................................\n",
            "Epoch 10/50\n",
            "  Train Loss: 0.6516 | Train Acc: 70.48%\n",
            "  Val   Loss: 0.7758 | Val   Acc: 68.39 | Val   Macro F1: 0.5971273271664019%\n",
            ".................................................\n",
            "Epoch 11/50\n",
            "  Train Loss: 0.6116 | Train Acc: 70.06%\n",
            "  Val   Loss: 0.7226 | Val   Acc: 70.40 | Val   Macro F1: 0.6478119635947975%\n",
            ".................................................\n",
            "Epoch 12/50\n",
            "  Train Loss: 0.5876 | Train Acc: 71.83%\n",
            "  Val   Loss: 0.7145 | Val   Acc: 69.73 | Val   Macro F1: 0.6193130625699632%\n",
            ".................................................\n",
            "Epoch 13/50\n",
            "  Train Loss: 0.5890 | Train Acc: 72.31%\n",
            "  Val   Loss: 0.7196 | Val   Acc: 72.20 | Val   Macro F1: 0.6575743273951073%\n",
            ".................................................\n",
            "Epoch 14/50\n",
            "  Train Loss: 0.5613 | Train Acc: 73.01%\n",
            "  Val   Loss: 0.7175 | Val   Acc: 70.63 | Val   Macro F1: 0.6285580051300133%\n",
            ".................................................\n",
            "Epoch 15/50\n",
            "  Train Loss: 0.5565 | Train Acc: 72.82%\n",
            "  Val   Loss: 0.7019 | Val   Acc: 72.65 | Val   Macro F1: 0.6636997254872825%\n",
            ".................................................\n",
            "Epoch 16/50\n",
            "  Train Loss: 0.5336 | Train Acc: 74.07%\n",
            "  Val   Loss: 0.6944 | Val   Acc: 71.75 | Val   Macro F1: 0.6710491790999719%\n",
            ".................................................\n",
            "Epoch 17/50\n",
            "  Train Loss: 0.5399 | Train Acc: 72.47%\n",
            "  Val   Loss: 0.6673 | Val   Acc: 70.63 | Val   Macro F1: 0.6556668289794791%\n",
            ".................................................\n",
            "Epoch 18/50\n",
            "  Train Loss: 0.5541 | Train Acc: 73.43%\n",
            "  Val   Loss: 0.7343 | Val   Acc: 70.18 | Val   Macro F1: 0.6167477593876565%\n",
            ".................................................\n",
            "Epoch 19/50\n",
            "  Train Loss: 0.5364 | Train Acc: 73.91%\n",
            "  Val   Loss: 0.6633 | Val   Acc: 71.30 | Val   Macro F1: 0.6648799858828022%\n",
            ".................................................\n",
            "Epoch 20/50\n",
            "  Train Loss: 0.5359 | Train Acc: 73.04%\n",
            "  Val   Loss: 0.6817 | Val   Acc: 74.22 | Val   Macro F1: 0.688476145452994%\n",
            ".................................................\n",
            "Epoch 21/50\n",
            "  Train Loss: 0.5148 | Train Acc: 73.62%\n",
            "  Val   Loss: 0.7127 | Val   Acc: 73.09 | Val   Macro F1: 0.6796827419255088%\n",
            ".................................................\n",
            "Epoch 22/50\n",
            "  Train Loss: 0.5109 | Train Acc: 75.42%\n",
            "  Val   Loss: 0.6826 | Val   Acc: 71.52 | Val   Macro F1: 0.6377822043656823%\n",
            ".................................................\n",
            "Epoch 23/50\n",
            "  Train Loss: 0.5237 | Train Acc: 74.13%\n",
            "  Val   Loss: 0.7451 | Val   Acc: 69.06 | Val   Macro F1: 0.6304324578633096%\n",
            ".................................................\n",
            "Epoch 24/50\n",
            "  Train Loss: 0.5014 | Train Acc: 75.61%\n",
            "  Val   Loss: 0.6604 | Val   Acc: 73.09 | Val   Macro F1: 0.6726739543211051%\n",
            ".................................................\n",
            "Epoch 25/50\n",
            "  Train Loss: 0.4925 | Train Acc: 75.32%\n",
            "  Val   Loss: 0.6530 | Val   Acc: 71.97 | Val   Macro F1: 0.6825712874826828%\n",
            ".................................................\n",
            "Epoch 26/50\n",
            "  Train Loss: 0.4740 | Train Acc: 75.51%\n",
            "  Val   Loss: 0.6561 | Val   Acc: 72.87 | Val   Macro F1: 0.679038425839977%\n",
            ".................................................\n",
            "Epoch 27/50\n",
            "  Train Loss: 0.4581 | Train Acc: 75.90%\n",
            "  Val   Loss: 0.6435 | Val   Acc: 71.97 | Val   Macro F1: 0.6696339402767175%\n",
            ".................................................\n",
            "Epoch 28/50\n",
            "  Train Loss: 0.4652 | Train Acc: 76.47%\n",
            "  Val   Loss: 0.6479 | Val   Acc: 74.22 | Val   Macro F1: 0.6985827757381641%\n",
            ".................................................\n",
            "Epoch 29/50\n",
            "  Train Loss: 0.4746 | Train Acc: 75.83%\n",
            "  Val   Loss: 0.6399 | Val   Acc: 72.20 | Val   Macro F1: 0.6926947360123688%\n",
            ".................................................\n",
            "Epoch 30/50\n",
            "  Train Loss: 0.4888 | Train Acc: 74.33%\n",
            "  Val   Loss: 0.6488 | Val   Acc: 74.44 | Val   Macro F1: 0.6878969538744999%\n",
            ".................................................\n",
            "Epoch 31/50\n",
            "  Train Loss: 0.4668 | Train Acc: 76.47%\n",
            "  Val   Loss: 0.6470 | Val   Acc: 73.77 | Val   Macro F1: 0.6872567509993679%\n",
            ".................................................\n",
            "Epoch 32/50\n",
            "  Train Loss: 0.4684 | Train Acc: 76.15%\n",
            "  Val   Loss: 0.6455 | Val   Acc: 72.87 | Val   Macro F1: 0.6927212982313486%\n",
            ".................................................\n",
            "Epoch 33/50\n",
            "  Train Loss: 0.4912 | Train Acc: 75.61%\n",
            "  Val   Loss: 0.6467 | Val   Acc: 72.42 | Val   Macro F1: 0.6896165814984088%\n",
            ".................................................\n",
            "Epoch 34/50\n",
            "  Train Loss: 0.4718 | Train Acc: 75.67%\n",
            "  Val   Loss: 0.6439 | Val   Acc: 73.99 | Val   Macro F1: 0.6897702827528137%\n",
            ".................................................\n",
            "Epoch 35/50\n",
            "  Train Loss: 0.4656 | Train Acc: 75.51%\n",
            "  Val   Loss: 0.6451 | Val   Acc: 74.22 | Val   Macro F1: 0.7020329934792865%\n",
            ".................................................\n",
            "Epoch 36/50\n",
            "  Train Loss: 0.4580 | Train Acc: 75.93%\n",
            "  Val   Loss: 0.6369 | Val   Acc: 72.87 | Val   Macro F1: 0.6932448934776787%\n",
            ".................................................\n",
            "Epoch 37/50\n",
            "  Train Loss: 0.4486 | Train Acc: 76.19%\n",
            "  Val   Loss: 0.6485 | Val   Acc: 72.87 | Val   Macro F1: 0.6918871555559811%\n",
            ".................................................\n",
            "Epoch 38/50\n",
            "  Train Loss: 0.4676 | Train Acc: 76.28%\n",
            "  Val   Loss: 0.6399 | Val   Acc: 74.22 | Val   Macro F1: 0.7013084568234165%\n",
            ".................................................\n",
            "Epoch 39/50\n",
            "  Train Loss: 0.4697 | Train Acc: 76.73%\n",
            "  Val   Loss: 0.6395 | Val   Acc: 73.99 | Val   Macro F1: 0.6978701400246574%\n",
            ".................................................\n",
            "Epoch 40/50\n",
            "  Train Loss: 0.4809 | Train Acc: 76.19%\n",
            "  Val   Loss: 0.6454 | Val   Acc: 73.54 | Val   Macro F1: 0.6867491026695007%\n",
            ".................................................\n",
            "Epoch 41/50\n",
            "  Train Loss: 0.4659 | Train Acc: 76.35%\n",
            "  Val   Loss: 0.6397 | Val   Acc: 72.87 | Val   Macro F1: 0.6927169905475832%\n",
            ".................................................\n",
            "Epoch 42/50\n",
            "  Train Loss: 0.4610 | Train Acc: 76.06%\n",
            "  Val   Loss: 0.6376 | Val   Acc: 72.42 | Val   Macro F1: 0.6899477089861241%\n",
            ".................................................\n",
            "Epoch 43/50\n",
            "  Train Loss: 0.4732 | Train Acc: 76.35%\n",
            "  Val   Loss: 0.6473 | Val   Acc: 73.09 | Val   Macro F1: 0.6929114357693572%\n",
            ".................................................\n",
            "Epoch 44/50\n",
            "  Train Loss: 0.4729 | Train Acc: 75.48%\n",
            "  Val   Loss: 0.6406 | Val   Acc: 73.54 | Val   Macro F1: 0.697861035340032%\n",
            ".................................................\n",
            "Epoch 45/50\n",
            "  Train Loss: 0.4673 | Train Acc: 75.90%\n",
            "  Val   Loss: 0.6402 | Val   Acc: 73.09 | Val   Macro F1: 0.6944413920788503%\n",
            ".................................................\n",
            "Epoch 46/50\n",
            "  Train Loss: 0.4637 | Train Acc: 76.60%\n",
            "  Val   Loss: 0.6383 | Val   Acc: 73.09 | Val   Macro F1: 0.6919243239457978%\n",
            ".................................................\n",
            "Epoch 47/50\n",
            "  Train Loss: 0.4591 | Train Acc: 75.83%\n",
            "  Val   Loss: 0.6364 | Val   Acc: 73.99 | Val   Macro F1: 0.6994842217393151%\n",
            ".................................................\n",
            "Epoch 48/50\n",
            "  Train Loss: 0.4755 | Train Acc: 75.74%\n",
            "  Val   Loss: 0.6395 | Val   Acc: 73.54 | Val   Macro F1: 0.6948766638482521%\n",
            ".................................................\n",
            "Epoch 49/50\n",
            "  Train Loss: 0.4686 | Train Acc: 76.57%\n",
            "  Val   Loss: 0.6407 | Val   Acc: 73.54 | Val   Macro F1: 0.6969940309596017%\n",
            ".................................................\n",
            "Epoch 50/50\n",
            "  Train Loss: 0.4438 | Train Acc: 76.99%\n",
            "  Val   Loss: 0.6445 | Val   Acc: 73.54 | Val   Macro F1: 0.6962861828256821%\n"
          ]
        }
      ],
      "source": [
        "num_classes = labels_df[label_col].nunique()\n",
        "model = resnet_model(num_classes=num_classes, freeze_backbone=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_df[label_col]), y=train_df[label_col])\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "train_criterion = nn.CrossEntropyLoss(class_weights)\n",
        "\n",
        "test_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
        "\n",
        "best_acc = 0.0\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_correct= train_one_epoch(model, train_loader, train_criterion, optimizer, device)\n",
        "    val_loss, val_correct, val_F1 = valid(model, val_loader, device, test_criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_correct:.2f}%\")\n",
        "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_correct:.2f}% | Val   Macro F1: {val_F1:.2f}%\")\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_F1 > best_acc:\n",
        "        best_acc = val_F1\n",
        "        torch.save(model.state_dict(), path + 'flat.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ci11a7Nkv5ZZ",
      "metadata": {
        "id": "Ci11a7Nkv5ZZ"
      },
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KhNNyfh5v8Yn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhNNyfh5v8Yn",
        "outputId": "02207588-31c7-41c2-925d-bce786cb7804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.7500    0.4615         4\n",
            "           1     0.6949    0.9111    0.7885        45\n",
            "           2     0.5833    0.7778    0.6667         9\n",
            "           3     0.8571    0.6729    0.7539       107\n",
            "           4     0.7023    0.6815    0.6917       135\n",
            "           5     0.7302    0.6970    0.7132       198\n",
            "           6     0.6646    0.7133    0.6881       150\n",
            "           7     0.8563    0.8232    0.8394       181\n",
            "           8     0.7808    0.9048    0.8382        63\n",
            "\n",
            "    accuracy                         0.7466       892\n",
            "   macro avg     0.6892    0.7702    0.7157       892\n",
            "weighted avg     0.7543    0.7466    0.7473       892\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#model = resnet_model(num_classes=num_classes, freeze_backbone=False)\n",
        "\n",
        "#model.load_state_dict(torch.load(path + 'flat.pth'))\n",
        "#resnet 18, stratified sampling, raw image data (pixel matric data)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_loss, correct = test(model, test_loader, device, test_criterion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}