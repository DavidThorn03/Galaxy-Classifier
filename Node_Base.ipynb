{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac",
      "metadata": {
        "id": "8f580b28-8f5f-4348-8f65-0aad3e700cac"
      },
      "source": [
        "## Classifier per Node Base Line Model\n",
        "with resnet18 and classifier per node"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6",
      "metadata": {
        "id": "6482c4d2-7cd7-471e-ba90-1931341dccb6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
      "metadata": {
        "id": "c1b56a40-00c6-40ed-ad6b-70cb3df237d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c60556-c04d-441b-d63b-d0a91a7feb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hiclass[all] in /usr/local/lib/python3.12/dist-packages (5.0.4)\n",
            "\u001b[33mWARNING: hiclass 5.0.4 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.5 in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from hiclass[all]) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5->hiclass[all]) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.5->hiclass[all]) (3.6.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install hiclass[all]\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import color\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from hiclass.metrics import f1 as hf1\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044c180a-d6d9-4bf8-9545-4f4147733a61",
      "metadata": {
        "id": "044c180a-d6d9-4bf8-9545-4f4147733a61"
      },
      "source": [
        "### LBP Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53000783-494e-44d7-88c7-0900c1fdfafc",
      "metadata": {
        "id": "53000783-494e-44d7-88c7-0900c1fdfafc"
      },
      "outputs": [],
      "source": [
        "class LBPTransform:\n",
        "    def __init__(self, radius=3, n_points=None, method='uniform'):\n",
        "        self.radius = radius\n",
        "        self.n_points = n_points if n_points else 8 * radius\n",
        "        self.method = method\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        if len(img.shape) == 3 :\n",
        "            gray = color.rgb2gray(img)\n",
        "        else:\n",
        "            gray = img\n",
        "\n",
        "        gray = (gray * 255).astype(np.uint8)\n",
        "\n",
        "        lbp = local_binary_pattern(gray, self.n_points, self.radius, self.method)\n",
        "\n",
        "        lbp = (lbp - lbp.min()) / (lbp.max() - lbp.min() + 1e-7)\n",
        "\n",
        "        lbp_3 = np.stack([lbp, lbp, lbp], axis=-1)\n",
        "\n",
        "        return lbp_3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1",
      "metadata": {
        "id": "2eec9685-e5ae-49e8-829f-0cbd70db40d1"
      },
      "source": [
        "### Make LBP Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1",
      "metadata": {
        "id": "af5f0d63-4d34-44ea-aa4e-d50df0b88af1"
      },
      "outputs": [],
      "source": [
        "def make_lbp_csv(input_folder, csv_path, lbp_transformer):\n",
        "    img_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
        "\n",
        "    print(f\"Processing {len(img_files)} images from {input_folder}...\")\n",
        "    with open(csv_path, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        header_written=False\n",
        "\n",
        "        for img, fname in enumerate(img_files):\n",
        "          in_path = os.path.join(input_folder, fname)\n",
        "          img = Image.open(in_path).convert('RGB')\n",
        "\n",
        "          img = lbp_transformer(img)\n",
        "          img = img.flatten()\n",
        "\n",
        "          if not header_written:\n",
        "            header = ['PGCname'] + [f'pixel_{i}' for i in range(len(img))]\n",
        "            writer.writerow(header)\n",
        "            header_written = True\n",
        "\n",
        "          writer.writerow([fname] + img.tolist)\n",
        "\n",
        "    print(\"LBP preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3",
      "metadata": {
        "id": "4556d2b0-340d-40b0-a13c-fa40daf91ce3"
      },
      "source": [
        "### Dataset Class\n",
        "Class for processing data and combining images with labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479",
      "metadata": {
        "id": "069073d5-c4b8-464d-ac66-2bcf6aee8479"
      },
      "outputs": [],
      "source": [
        "class PGCDataset(Dataset):\n",
        "    def __init__(self, labels_df, img_folder, id_col='PGCname', label_col='T', transform=None):\n",
        "        self.labels_df = labels_df.reset_index(drop=True)\n",
        "        self.img_folder = img_folder\n",
        "        self.id_col = id_col\n",
        "        self.label_col = label_col\n",
        "        self.transform = transform\n",
        "\n",
        "        available_imgs = {f.replace('.png', '') for f in os.listdir(img_folder)\n",
        "                            if f.endswith('.png')}\n",
        "        self.labels_df = self.labels_df[self.labels_df[id_col].isin(available_imgs)].reset_index(drop=True)\n",
        "\n",
        "        print(f\"Dataset created with {len(self.labels_df)} imgs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.labels_df.iloc[idx]\n",
        "\n",
        "        img_id = row[self.id_col]\n",
        "        img_path = os.path.join(self.img_folder, f\"{img_id}.png\")\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        label = torch.tensor(int(row[self.label_col]), dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label, img_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5",
      "metadata": {
        "id": "1c04eb15-ac7d-4317-b24a-66a69a4d59f5"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4244d3f0-97f7-43a9-b040-a0384a418bd2",
        "outputId": "8658ebd8-77e6-43e8-e6ce-a6eca9ed2a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 3120 imgs\n",
            "Dataset created with 446 imgs\n",
            "Dataset created with 892 imgs\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/Othercomputers/My laptop/Thesis/Galaxy-Classifier/'\n",
        "img_folder = path + '/images'\n",
        "lbp_img_folder = path + '/lbp_images'\n",
        "\n",
        "id_col = 'PGCname'\n",
        "label_col = 'T'\n",
        "\n",
        "img_size = 224\n",
        "\n",
        "lbp_trans = LBPTransform(5)\n",
        "\n",
        "labels_df = pd.read_csv(path + 'EFIGI_attributes.txt', sep=r'\\s+', comment='#')\n",
        "labels_df[label_col] = labels_df[label_col].replace({-3:-2, -1:-2}) # S0\n",
        "labels_df[label_col] = labels_df[label_col].replace({0:1, 2:1}) # Sa\n",
        "labels_df[label_col] = labels_df[label_col].replace({3:4}) # Sb\n",
        "labels_df[label_col] = labels_df[label_col].replace({5:6}) # Sc\n",
        "labels_df[label_col] = labels_df[label_col].replace({8:7, 9:7}) # Sd\n",
        "labels_df[label_col] = labels_df[label_col].replace({10:11}) # Irr\n",
        "\n",
        "labels_df[label_col] = labels_df[label_col].replace({-6:0, -5:1, -4:2, -2:3, 1:4, 4:5, 11:8}) # Adjust to 0 - 8\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=0, stratify=labels_df[label_col])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.125, random_state=0, stratify=train_df[label_col])\n",
        "\n",
        "# use stratify sampling in training - write to csv file - - tocsv.pandas\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(180),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\"\"\"lbp_params = {'radius': 3, 'n_points': 24, 'method': 'uniform'}\n",
        "\n",
        "lbp_transform = LBPTransform(**lbp_params)\n",
        "\n",
        "make_lbp_images(img_folder, lbp_img_folder, lbp_transform)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_dataset = PGCDataset(\n",
        "    labels_df=train_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=train_transform\n",
        ")\n",
        "val_dataset = PGCDataset(\n",
        "    labels_df=val_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")\n",
        "test_dataset = PGCDataset(\n",
        "    labels_df=test_df,\n",
        "    img_folder=img_folder,\n",
        "    id_col=id_col,\n",
        "    label_col=label_col,\n",
        "    transform=test_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62",
      "metadata": {
        "id": "3e26d54c-8ece-4ca0-9511-ab7ce4c26a62"
      },
      "source": [
        "### Data loader\n",
        "loads data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26",
      "metadata": {
        "id": "c7ddb34a-a3e8-41b9-9a24-fb0d103c0b26"
      },
      "outputs": [],
      "source": [
        "labels = train_df[label_col].values\n",
        "classes= np.unique(labels)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
        "\n",
        "sample_weights = np.array([class_weights[np.where(classes == label)[0][0]] for label in labels])\n",
        "sample_weights = torch.from_numpy(sample_weights).float()\n",
        "\n",
        "#sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True) # worse for underrepresented classes and overall\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed",
      "metadata": {
        "id": "cb3fa9ab-95ec-4320-bbd7-c4215834aaed"
      },
      "source": [
        "### Hierarchical model\n",
        "using pretrained resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146c13bc-506b-49d8-8b6e-095e8e170259",
      "metadata": {
        "id": "146c13bc-506b-49d8-8b6e-095e8e170259"
      },
      "outputs": [],
      "source": [
        "class HierarchicalResNet(nn.Module):\n",
        "  def __init__(self, num_coarse, fine_per_coarse, freeze_backbone=False):\n",
        "    super(HierarchicalResNet, self).__init__()\n",
        "    # get model\n",
        "    resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
        "    # freeze layers\n",
        "    for param in resnet.parameters():\n",
        "      param.requires_grad = not freeze_backbone\n",
        "\n",
        "    # remove the final full connect layer\n",
        "    self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "    in_features = resnet.fc.in_features\n",
        "\n",
        "    self.num_coarse = num_coarse\n",
        "    self.fine_per_coarse = fine_per_coarse\n",
        "\n",
        "    # classifier heads\n",
        "    self.coarse_classifier = nn.Sequential(nn.Linear(in_features, num_coarse))\n",
        "    self.fine_classifiers = nn.ModuleList([nn.Linear(in_features, num_fine) for num_fine in fine_per_coarse])\n",
        "\n",
        "    self.fine_to_coarse = {}\n",
        "    self.coarse_to_fine = {}\n",
        "\n",
        "    # build mapping\n",
        "    fine_id = 0\n",
        "\n",
        "    for coarse_id, num_fine in enumerate(fine_per_coarse):\n",
        "      self.coarse_to_fine[coarse_id] = []\n",
        "      for local_fine_id in range(num_fine):\n",
        "        self.fine_to_coarse[fine_id] = (coarse_id, local_fine_id)\n",
        "        self.coarse_to_fine[coarse_id].append(fine_id)\n",
        "        fine_id += 1\n",
        "\n",
        "    self.total_fine_classes = sum(fine_per_coarse)\n",
        "\n",
        "\n",
        "  def forward(self, x, return_type='train'):\n",
        "    features = self.backbone(x)\n",
        "    features = torch.flatten(features, 1)\n",
        "\n",
        "    # get predictions from features\n",
        "    coarse_output = self.coarse_classifier(features)\n",
        "    fine_output = [classifier(features) for classifier in self.fine_classifiers]\n",
        "\n",
        "    if return_type == 'train':\n",
        "      return coarse_output, fine_output, features\n",
        "\n",
        "    elif return_type == 'test':\n",
        "      return self.joint_prob(coarse_output, fine_output)\n",
        "\n",
        "\n",
        "  def joint_prob(self, coarse_output, fine_output):\n",
        "    batch = coarse_output.size(0)\n",
        "    device = coarse_output.device\n",
        "\n",
        "    coarse_probs = F.softmax(coarse_output, dim=1)\n",
        "\n",
        "    fine_cond_probs = [\n",
        "        F.softmax(logits, dim=1) for logits in fine_output\n",
        "    ]\n",
        "\n",
        "    fine_probs = torch.zeros((batch, self.total_fine_classes), device=device)\n",
        "\n",
        "    fine_id = 0\n",
        "    for coarse_id, cond_probs in enumerate(fine_cond_probs):\n",
        "      num_fine_in_coarse = cond_probs.size(1)\n",
        "\n",
        "      coarse_probs_per_class = coarse_probs[:, coarse_id].unsqueeze(1)\n",
        "      joint_prob = coarse_probs_per_class * cond_probs\n",
        "\n",
        "      fine_probs[:, fine_id:fine_id + num_fine_in_coarse] = joint_prob\n",
        "\n",
        "      fine_id += num_fine_in_coarse\n",
        "\n",
        "    return coarse_probs, fine_probs, fine_cond_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical Loss\n",
        "using cross entropy"
      ],
      "metadata": {
        "id": "zWmIxHc3JvDE"
      },
      "id": "zWmIxHc3JvDE"
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalLoss(nn.Module):\n",
        "  def __init__(self, fine_per_coarse, fine_weights=None, coarse_weights=None):\n",
        "    super(HierarchicalLoss, self).__init__()\n",
        "    # hash map of fine - course\n",
        "    self.fine_per_coarse = fine_per_coarse\n",
        "\n",
        "    self.loss_coarse = nn.CrossEntropyLoss(weight=coarse_weights)\n",
        "\n",
        "    if fine_weights is not None:\n",
        "      self.loss_fine = nn.ModuleList()\n",
        "      start_id = 0\n",
        "      for num_fine in fine_per_coarse:\n",
        "        coarse_weights = fine_weights[start_id:start_id + num_fine]\n",
        "        self.loss_fine.append(nn.CrossEntropyLoss(weight=coarse_weights))\n",
        "        start_id += num_fine\n",
        "    else:\n",
        "      self.loss_fine = nn.ModuleList([nn.CrossEntropyLoss() for _ in fine_per_coarse])\n",
        "\n",
        "    # tensor for mapping\n",
        "    self.fine_to_coarse = {}\n",
        "    fine_id = 0\n",
        "    for coarse_id, num_fine in enumerate(fine_per_coarse):\n",
        "      for local_fine_id in range(num_fine):\n",
        "        self.fine_to_coarse[fine_id] = (coarse_id, local_fine_id)\n",
        "        fine_id += 1\n",
        "\n",
        "\n",
        "  def forward(self, coarse_logits, fine_logits, fine_labels):\n",
        "    batch = fine_labels.size(0)\n",
        "    device = fine_labels.device\n",
        "\n",
        "    coarse_labels = torch.zeros(batch, dtype=torch.long, device=device)\n",
        "    local_fine = torch.zeros(batch, dtype=torch.long, device=device)\n",
        "\n",
        "    for i, global_fine in enumerate(fine_labels):\n",
        "      coarse_id, local_fine_id = self.fine_to_coarse[global_fine.item()]\n",
        "      coarse_labels[i] = coarse_id\n",
        "      local_fine[i] = local_fine_id\n",
        "\n",
        "    coarse_loss = self.loss_coarse(coarse_logits, coarse_labels)\n",
        "\n",
        "    fine_loss = 0\n",
        "    for i in range(batch):\n",
        "      coarse_id = coarse_labels[i].item()\n",
        "\n",
        "      local_fine_logits = fine_logits[coarse_id][i].unsqueeze(0)\n",
        "      fine_label = local_fine[i].unsqueeze(0)\n",
        "      fine_loss += self.loss_fine[coarse_id](local_fine_logits, fine_label)\n",
        "\n",
        "    fine_loss /= batch\n",
        "\n",
        "    loss = coarse_loss + fine_loss\n",
        "\n",
        "    return loss, coarse_loss, fine_loss\n"
      ],
      "metadata": {
        "id": "MKewVoSLJmK2"
      },
      "id": "MKewVoSLJmK2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate class weights"
      ],
      "metadata": {
        "id": "xd3-ttA8KBK3"
      },
      "id": "xd3-ttA8KBK3"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(labels, fine_per_coarse, device='cpu'):\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    fine_to_coarse_mapping = {}\n",
        "    fine_id = 0\n",
        "    for coarse_id, num_fine in enumerate(fine_per_coarse):\n",
        "      for local_fine_id in range(num_fine):\n",
        "        fine_to_coarse_mapping[fine_id] = coarse_id\n",
        "        fine_id += 1\n",
        "\n",
        "    fine_classes = np.unique(labels)\n",
        "    fine_weights = compute_class_weight('balanced', classes=fine_classes, y=labels)\n",
        "\n",
        "    total_fine_classes = sum(fine_per_coarse)\n",
        "    fine_weights_full = torch.ones(total_fine_classes, dtype=torch.float, device=device)\n",
        "    for i, class_id in enumerate(fine_classes):\n",
        "        fine_weights_full[class_id] = fine_weights[i]\n",
        "\n",
        "    coarse_labels = np.array([fine_to_coarse_mapping[label] for label in labels])\n",
        "    coarse_classes = np.unique(coarse_labels)\n",
        "    coarse_weights = compute_class_weight('balanced', classes=coarse_classes, y=coarse_labels)\n",
        "\n",
        "    num_coarse_classes = len(fine_per_coarse)\n",
        "    coarse_weights_full = torch.ones(num_coarse_classes, dtype=torch.float, device=device)\n",
        "    for i, class_id in enumerate(coarse_classes):\n",
        "        coarse_weights_full[class_id] = coarse_weights[i]\n",
        "\n",
        "    return fine_weights_full, coarse_weights_full"
      ],
      "metadata": {
        "id": "B0JYvXK2KHZr"
      },
      "id": "B0JYvXK2KHZr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical F1 calculation"
      ],
      "metadata": {
        "id": "Hl3YPWGJgqHi"
      },
      "id": "Hl3YPWGJgqHi"
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_f1_per_class(true_hier, pred_hier, num_coarse, num_fine):\n",
        "    class_indices = defaultdict(list)\n",
        "    for i, (c, f) in enumerate(true_hier):\n",
        "        class_indices[(c, f)].append(i)\n",
        "\n",
        "    f1_per_class = {}\n",
        "\n",
        "    for (c, f), idxs in class_indices.items():\n",
        "        if len(idxs) == 0:\n",
        "            f1_per_class[(c, f)] = 0.0\n",
        "            continue\n",
        "\n",
        "        t_subset = [true_hier[i] for i in idxs]\n",
        "        p_subset = [pred_hier[i] for i in idxs]\n",
        "\n",
        "        score = hf1(t_subset, p_subset)\n",
        "        f1_per_class[(c, f)] = score\n",
        "\n",
        "    return f1_per_class\n"
      ],
      "metadata": {
        "id": "lVaD5cpCgyk7"
      },
      "id": "lVaD5cpCgyk7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377",
      "metadata": {
        "id": "2ea6a124-037f-40b4-8fe1-9d2befb73377"
      },
      "source": [
        "## Train/test model methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee",
      "metadata": {
        "id": "0461f6ae-cb64-45ca-8c09-9b12f919b8ee"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, running_coarse, running_fine = 0.0, 0.0, 0.0\n",
        "    correct_coarse, correct_fine = 0, 0\n",
        "    total = 0\n",
        "\n",
        "    for img, labels, ids in dataloader:\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        coarse_logits, fine_logits, _ = model(img, return_type='train')\n",
        "\n",
        "        loss, coarse_loss, fine_loss = criterion(coarse_logits, fine_logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_coarse += coarse_loss.item()\n",
        "        running_fine += fine_loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          coarse_probs, fine_probs, _ = model(img, return_type='test')\n",
        "          predict_coarse = coarse_probs.argmax(1)\n",
        "          predict_fine = fine_probs.argmax(1)\n",
        "\n",
        "          coarse_labels = torch.tensor([\n",
        "              criterion.fine_to_coarse[label.item()][0]\n",
        "              for label in labels\n",
        "          ], device=device)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct_fine += (predict_fine == labels).sum().item()\n",
        "        correct_coarse += (predict_coarse == coarse_labels).sum().item()\n",
        "\n",
        "\n",
        "        print(\".\", end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_coarse = running_coarse / len(dataloader)\n",
        "    epoch_fine = running_fine / len(dataloader)\n",
        "\n",
        "    epoch_acc_fine = 100.0 * correct_fine / total\n",
        "    epoch_acc_coarse = 100.0 * correct_coarse / total\n",
        "\n",
        "    return {\n",
        "        'total_loss': epoch_loss,\n",
        "        'coarse_loss': epoch_coarse,\n",
        "        'fine_loss': epoch_fine,\n",
        "        'fine_acc': epoch_acc_fine,\n",
        "        'coarse_acc': epoch_acc_coarse\n",
        "    }\n",
        "\n",
        "def valid(model, dataloader, device, loss_fn, use_weighted=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    correct_fine, correct_coarse = 0, 0\n",
        "\n",
        "    all_preds_fine = []\n",
        "    all_preds_coarse = []\n",
        "    all_labels_fine = []\n",
        "    all_labels_coarse = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y, _ in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "\n",
        "            coarse_logits, fine_logits, _ = model(X, return_type='train')\n",
        "\n",
        "            loss, _, _ = loss_fn(coarse_logits, fine_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            coarse_probs, fine_probs, _ = model(X, return_type='test')\n",
        "\n",
        "            predicted_coarse = coarse_probs.argmax(1)\n",
        "            predicted_fine = fine_probs.argmax(1)\n",
        "\n",
        "            coarse_labels = torch.tensor([\n",
        "                loss_fn.fine_to_coarse[label.item()][0]\n",
        "                for label in y\n",
        "            ], device=device)\n",
        "\n",
        "            correct_fine += (predicted_fine == y).type(torch.float).sum().item()\n",
        "            correct_coarse += (predicted_coarse == coarse_labels).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds_fine.extend(predicted_fine.cpu().numpy())\n",
        "            all_preds_coarse.extend(predicted_coarse.cpu().numpy())\n",
        "            all_labels_fine.extend(y.cpu().numpy())\n",
        "            all_labels_coarse.extend(coarse_labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct_fine /= size\n",
        "    correct_coarse /= size\n",
        "\n",
        "    fine_report = classification_report(all_labels_fine, all_preds_fine, digits=4, output_dict=True, zero_division=0)\n",
        "    coarse_report = classification_report(all_labels_coarse, all_preds_coarse, digits=4, output_dict=True, zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'total_loss': test_loss,\n",
        "        'fine_acc': correct_fine * 100,\n",
        "        'coarse_acc': correct_coarse * 100,\n",
        "        'fine_F1': fine_report['macro avg']['f1-score'] * 100,\n",
        "        'coarse_F1': coarse_report['macro avg']['f1-score'] * 100,\n",
        "        'predictions': {\n",
        "            'fine': all_preds_fine,\n",
        "            'coarse': all_preds_coarse,\n",
        "            'labels_fine': all_labels_fine,\n",
        "            'labels_coarse': all_labels_coarse\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def test(model, dataloader, device, loss_fn, use_weighted=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    correct_fine, correct_coarse = 0, 0\n",
        "\n",
        "    all_preds_fine = []\n",
        "    all_preds_coarse = []\n",
        "    all_labels_fine = []\n",
        "    all_labels_coarse = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y, _ in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "\n",
        "            coarse_logits, fine_logits, _ = model(X, return_type='train')\n",
        "\n",
        "            loss, _, _ = loss_fn(coarse_logits, fine_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            coarse_probs, fine_probs, _ = model(X, return_type='test')\n",
        "\n",
        "            predicted_coarse = coarse_probs.argmax(1)\n",
        "            predicted_fine = fine_probs.argmax(1)\n",
        "\n",
        "            coarse_labels = torch.tensor([\n",
        "                loss_fn.fine_to_coarse[label.item()][0]\n",
        "                for label in y\n",
        "            ], device=device)\n",
        "\n",
        "            correct_fine += (predicted_fine == y).type(torch.float).sum().item()\n",
        "            correct_coarse += (predicted_coarse == coarse_labels).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds_fine.extend(predicted_fine.cpu().numpy())\n",
        "            all_preds_coarse.extend(predicted_coarse.cpu().numpy())\n",
        "            all_labels_fine.extend(y.cpu().numpy())\n",
        "            all_labels_coarse.extend(coarse_labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct_fine /= size\n",
        "    correct_coarse /= size\n",
        "\n",
        "    print(\"Fine classes\")\n",
        "    print(classification_report(all_labels_fine, all_preds_fine, digits=4))\n",
        "\n",
        "    print(\"Coarse classes\")\n",
        "    print(classification_report(all_labels_coarse, all_preds_coarse, digits=4))\n",
        "\n",
        "    true_hierarchical = []\n",
        "    pred_hierarchical = []\n",
        "\n",
        "    for i in range(len(all_labels_fine)):\n",
        "      true_fine = all_labels_fine[i]\n",
        "      true_coarse = all_labels_coarse[i]\n",
        "\n",
        "      true_hierarchical.append([true_coarse, true_fine])\n",
        "\n",
        "      pred_fine = all_preds_fine[i]\n",
        "      pred_coarse = all_preds_coarse[i]\n",
        "\n",
        "      pred_hierarchical.append([pred_coarse, pred_fine])\n",
        "\n",
        "    h_f1_per_class = hierarchical_f1_per_class(\n",
        "        true_hierarchical,\n",
        "        pred_hierarchical,\n",
        "        num_coarse,\n",
        "        num_fine\n",
        "    )\n",
        "\n",
        "    print(\"Hierarchical F1\")\n",
        "    for cls, score in h_f1_per_class.items():\n",
        "        print(f\"Class coarse={cls[0]}, fine={cls[1]}: {score * 100:.2f}\")\n",
        "\n",
        "\n",
        "    h_f1 = hf1(true_hierarchical, pred_hierarchical)\n",
        "    print(f\"Hierarchical Macro Avg F1: {h_f1 * 100:.2f}\")\n",
        "\n",
        "    return {\n",
        "        'total_loss': test_loss,\n",
        "        'fine_acc': correct_fine * 100,\n",
        "        'coarse_acc': correct_coarse * 100,\n",
        "        'h_f1': h_f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "sRDJ8Ph-vuwQ"
      },
      "id": "sRDJ8Ph-vuwQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46069f7-c98f-4c8d-845a-e577489d23fd",
        "outputId": "53f50e54-c740-43a3-887a-842b24fa995e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "model_name = 'parent_base.pth'\n",
        "num_coarse = 4\n",
        "num_fine = 9\n",
        "\n",
        "fine_per_coarse = [3, 1, 4, 1]\n",
        "\n",
        "model = HierarchicalResNet(num_coarse, fine_per_coarse, freeze_backbone=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "fine_weights, coarse_weights = calculate_class_weights(train_df[label_col], fine_per_coarse, device)\n",
        "\n",
        "train_criterion = HierarchicalLoss(fine_per_coarse, fine_weights, coarse_weights)\n",
        "test_criterion = HierarchicalLoss(fine_per_coarse)\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
        "\n",
        "best_acc = 0.0\n",
        "\n",
        "epochs = 0\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    train_metrics = train_one_epoch(model, train_loader, train_criterion, optimizer, device)\n",
        "    print(f\"Train - Loss: {train_metrics['total_loss']:.4f}, \"\n",
        "          f\"Fine Acc: {train_metrics['fine_acc']:.2f}%, \"\n",
        "          f\"Coarse Acc: {train_metrics['coarse_acc']:.2f}%\")\n",
        "\n",
        "    valid_metrics = valid(model, val_loader, device, test_criterion)\n",
        "    print(f\"Valid - Loss: {valid_metrics['total_loss']:.4f}, \"\n",
        "          f\"Fine Acc: {valid_metrics['fine_acc']:.2f}% | Fine F1: {valid_metrics['fine_F1']:.2f}%, \"\n",
        "          f\"Coarse Acc: {valid_metrics['coarse_acc']:.2f}% | Coarse F1: {valid_metrics['coarse_F1']:.2f}%\")\n",
        "\n",
        "    scheduler.step(valid_metrics['total_loss'])\n",
        "\n",
        "    if valid_metrics['fine_F1'] > best_acc:\n",
        "        best_acc = valid_metrics['fine_F1']\n",
        "        torch.save(model.state_dict(), path + model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model"
      ],
      "metadata": {
        "id": "Ci11a7Nkv5ZZ"
      },
      "id": "Ci11a7Nkv5ZZ"
    },
    {
      "cell_type": "code",
      "source": [
        "model = HierarchicalResNet(num_coarse, fine_per_coarse, freeze_backbone=True)\n",
        "\n",
        "model.load_state_dict(torch.load(path + model_name))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_metrics = test(model, test_loader, device, test_criterion)\n"
      ],
      "metadata": {
        "id": "KhNNyfh5v8Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f679859-8a88-4d1a-ed67-5871503a13b1"
      },
      "id": "KhNNyfh5v8Yn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7500    0.7500    0.7500         4\n",
            "           1     0.7213    0.9778    0.8302        45\n",
            "           2     0.8333    0.5556    0.6667         9\n",
            "           3     0.7500    0.8411    0.7930       107\n",
            "           4     0.7938    0.5704    0.6638       135\n",
            "           5     0.7157    0.7374    0.7264       198\n",
            "           6     0.6543    0.7067    0.6795       150\n",
            "           7     0.8642    0.7735    0.8163       181\n",
            "           8     0.7500    0.9048    0.8201        63\n",
            "\n",
            "    accuracy                         0.7489       892\n",
            "   macro avg     0.7592    0.7575    0.7495       892\n",
            "weighted avg     0.7555    0.7489    0.7466       892\n",
            "\n",
            "Coarse classes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7778    0.9655    0.8615        58\n",
            "           1     0.7563    0.8411    0.7965       107\n",
            "           2     0.9872    0.9322    0.9589       664\n",
            "           3     0.7568    0.8889    0.8175        63\n",
            "\n",
            "    accuracy                         0.9204       892\n",
            "   macro avg     0.8195    0.9069    0.8586       892\n",
            "weighted avg     0.9296    0.9204    0.9231       892\n",
            "\n",
            "Hierarchical F1\n",
            "Class coarse=2, fine=7: 83.70\n",
            "Class coarse=3, fine=8: 89.68\n",
            "Class coarse=2, fine=4: 68.52\n",
            "Class coarse=2, fine=5: 86.87\n",
            "Class coarse=2, fine=6: 85.33\n",
            "Class coarse=1, fine=3: 90.40\n",
            "Class coarse=0, fine=1: 98.89\n",
            "Class coarse=0, fine=2: 72.22\n",
            "Class coarse=0, fine=0: 88.89\n",
            "Hierarchical Macro Avg F1: 84.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "new Audio('https://actions.google.com/sounds/v1/alarms/beep_short.ogg').play();"
      ],
      "metadata": {
        "id": "nYhFzCkL8oQ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "131a0522-b950-4b30-b4bb-b25d1bf3e950"
      },
      "id": "nYhFzCkL8oQ6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "new Audio('https://actions.google.com/sounds/v1/alarms/beep_short.ogg').play();\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}